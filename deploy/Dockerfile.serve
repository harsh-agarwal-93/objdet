# Serving container for ObjDet
# Optimized for inference with LitServe

# =============================================================================
# Base stage with CUDA runtime (smaller than full CUDA)
# =============================================================================
FROM nvidia/cuda:12.2.2-cudnn8-runtime-ubuntu22.04 AS base

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Install uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# Install system dependencies (minimal for serving)
# We do not need python here as we install it via uv
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Install Python via uv
COPY .python-version .
RUN uv python install

# =============================================================================
# Dependencies stage
# =============================================================================
FROM base AS dependencies

# Copy dependency files
COPY pyproject.toml uv.lock LICENSE README.md ./

# Copy ML source code for local package installation
COPY ml/src/ ml/src/

# Install dependencies (no dev, no docs, include tensorrt for inference)
RUN uv sync --frozen --no-dev --no-editable --extra tensorrt

# =============================================================================
# Final stage
# =============================================================================
FROM base AS final

# Copy installed dependencies from dependencies stage
COPY --from=dependencies /app/.venv /app/.venv

# Set up virtual environment
ENV PATH="/app/.venv/bin:$PATH"
ENV VIRTUAL_ENV="/app/.venv"

# Copy only necessary source code for serving
COPY ml/src/objdet/core/ ml/src/objdet/core/
COPY ml/src/objdet/models/ ml/src/objdet/models/
COPY ml/src/objdet/inference/ ml/src/objdet/inference/
COPY ml/src/objdet/serving/ ml/src/objdet/serving/
COPY ml/src/objdet/optimization/ ml/src/objdet/optimization/
COPY ml/src/objdet/__init__.py ml/src/objdet/__init__.py
COPY ml/src/objdet/version.py ml/src/objdet/version.py
COPY ml/configs/serving/ ml/configs/serving/

ENV PYTHONPATH="${PYTHONPATH}:/app/ml/src"

# Create non-root user for security
RUN useradd --create-home --shell /bin/bash appuser && \
    chown -R appuser:appuser /app

# Create directory for models
RUN mkdir -p /app/models && chown appuser:appuser /app/models

USER appuser

# Expose LitServe port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Default command starts serving
ENTRYPOINT ["python", "-m", "objdet"]
CMD ["serve", "--config", "configs/serving/default.yaml"]

# Labels
LABEL org.opencontainers.image.title="ObjDet Serving"
LABEL org.opencontainers.image.description="Object detection inference API"
LABEL org.opencontainers.image.source="https://github.com/harsh-agarwal-93/objdet"
LABEL org.opencontainers.image.licenses="Apache-2.0"
