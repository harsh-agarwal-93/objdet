# Docker Compose for local development
# Includes RabbitMQ for job queue and MLflow for experiment tracking

services:
  # ==========================================================================
  # Core Services
  # ==========================================================================

  # RabbitMQ for Celery message broker
  rabbitmq:
    image: rabbitmq:3.13-management-alpine
    container_name: objdet-rabbitmq
    ports:
      - "5672:5672"   # AMQP protocol
      - "15672:15672" # Management UI
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-objdet}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASS:-objdet_dev}
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    healthcheck:
      test: rabbitmq-diagnostics check_port_connectivity
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # MLflow tracking server
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.10.0
    container_name: objdet-mlflow
    ports:
      - "5000:5000"
    environment:
      MLFLOW_BACKEND_STORE_URI: sqlite:///mlflow.db
      MLFLOW_DEFAULT_ARTIFACT_ROOT: /mlartifacts
    volumes:
      - mlflow_data:/mlflow.db
      - mlflow_artifacts:/mlartifacts
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow.db
      --default-artifact-root /mlartifacts
      --host 0.0.0.0
      --port 5000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # ==========================================================================
  # Application Services
  # ==========================================================================

  # Celery worker for processing training jobs
  celery-worker:
    build:
      context: .
      dockerfile: Dockerfile.train
    container_name: objdet-celery-worker
    command: >
      celery -A objdet.pipelines.celery_app worker
      --loglevel=info
      --concurrency=1
    environment:
      CELERY_BROKER_URL: amqp://${RABBITMQ_USER:-objdet}:${RABBITMQ_PASS:-objdet_dev}@rabbitmq:5672//
      CELERY_RESULT_BACKEND: rpc://
      MLFLOW_TRACKING_URI: http://mlflow:5000
    volumes:
      - ./data:/app/data:ro
      - ./checkpoints:/app/checkpoints
      - ./configs:/app/configs:ro
    depends_on:
      rabbitmq:
        condition: service_healthy
      mlflow:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  # Inference server
  serve:
    build:
      context: .
      dockerfile: Dockerfile.serve
    container_name: objdet-serve
    ports:
      - "8000:8000"
    environment:
      MLFLOW_TRACKING_URI: http://mlflow:5000
    volumes:
      - ./models:/app/models:ro
      - ./configs/serving:/app/configs/serving:ro
    depends_on:
      mlflow:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

volumes:
  rabbitmq_data:
  mlflow_data:
  mlflow_artifacts:

networks:
  default:
    name: objdet-network
